{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chime, warnings,time\n",
    "chime.notify_exceptions()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Dependencies for the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science basic imports\n",
    "import pandas as pd, numpy as np, seaborn as sb, matplotlib.pyplot as plt, matplotlib.dates as mdates\n",
    "# ML Data Preprocessing imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Z-Score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding - Data Understanding - Data Preparation - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'datasc-660x434.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business problem is understanding the COVID-19.The cumulative count of confirmed, death and recovered cases of COVID-19 from different countries from 22nd January 2020\n",
    "1. Collecting data https://www.kaggle.com/datasets/imdevskp/corona-virus-report?select=covid_19_clean_complete.csv\n",
    "- [Note: the downloaded file is in .zip, we need to unzip it then import it as csv]\n",
    "2. Understanding the data by using the traditional Python data science modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will walkthrough the pandas library which is used for data wrangling tasks: data assessing, data coding and data cleaning. This is a part of business & data understanding and preparation. [Note: Best practice is always to import os module along with numpy]\n",
    "\n",
    "- Importing data and dealing with data with different formats: CSV, JSON, TSV, XLSX, TXT, HTML and etc ...\n",
    "  - pd.read_csv('FILENAME.CSV')\n",
    "  - pd.read_excel('FILENAME.XLSX',sheet = n) # where n is the sheet number \n",
    "- Functions and command to assess the data\n",
    "  - head(n): Finding n top rows\n",
    "  - tail(n): Finding n bottom rows\n",
    "  - sample(n): Finding n sample of data\n",
    "  - info(): get an overall information about the data attributes/types/missing values\n",
    "  - shape: finding the shape of data in rows x columns\n",
    "  - describe(): find statistical measures about your data including: mean, std, Q1,2,3,4 and range, and mode and median.\n",
    "  - plot(): plotting the data\n",
    "- Creating a data frame or a data series\n",
    "  - DataFrame({'COL1_NAME': [Values],'COL2_NAME': [Values]})\n",
    "  - Series({'INDEX_NAME': [Values]})\n",
    "- Important functions and cheatsheet check [Cheatsheet](./Pandas_Cheat_Sheet.pdf)\n",
    "  - lambda function\n",
    "  - join\n",
    "  - melt\n",
    "  - merge\n",
    "  - concatenate\n",
    "  and etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Process [Data Understanding & Data Preparation]\n",
    "- Importing the data and the required modules\n",
    "- Assessing the data\n",
    "- Documenting the observations\n",
    "- Taking action and decision regarding wrangling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping the data file since it is a zip file we add attribute compression = 'zip'\n",
    "file_name = 'covid_19_clean_complete.csv.zip'\n",
    "df = pd.read_csv(file_name ,compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',None,'display.max_colwidth',None,'display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Province/State'].isna().sum()\n",
    "# it has a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()] # Find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. 10 Columns and Province/State has many 35000 null value.\n",
    "2. The date variable is an object format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy (If province/state not needed):\n",
    "1. Drop the Province/State\n",
    "2. Change the date column to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Province/State column\n",
    "df.drop('Province/State',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG = df.groupby('Country/Region')[['Confirmed', 'Deaths','Recovered', 'Active']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis [EDA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(df['Date'],df['Deaths'])\n",
    "plt.xlabel('Date',fontweight='bold')\n",
    "plt.ylabel('Number of Deaths',fontweight='bold')\n",
    "plt.title('Number of Deaths over time')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG = AGG.sort_values(by='Deaths',ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q&A:\n",
    "\n",
    "1. Number of death Top 10\n",
    "2. Number of death by date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=AGG.index,height=AGG['Deaths'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEATHS_BY_DATE = df.groupby('Date')['Deaths'].sum()\n",
    "DEATHS_BY_DATE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating the plot with the user's data\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "# Plotting the data\n",
    "ax.plot_date(DEATHS_BY_DATE.index, DEATHS_BY_DATE.values, linestyle='solid', marker=None)\n",
    "# Annotate each point with the value using the last day of each month\n",
    "for date in DEATHS_BY_DATE.index:\n",
    "    if date.is_month_end:\n",
    "        ax.annotate(f'{DEATHS_BY_DATE[date]:,}', \n",
    "                    (mdates.date2num(date), DEATHS_BY_DATE[date]), \n",
    "                    xytext=(0,5), \n",
    "                    textcoords='offset points', \n",
    "                    ha='center')\n",
    "# Set grid on\n",
    "ax.grid(True)\n",
    "plt.title('Actual Data of Deaths')\n",
    "# Rotate and align the tick labels so they look better\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling [Machine Learning Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = basic_modelling1.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Futuristic Question 1: How many deaths expected in the next week?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we ask ourselves a question in the future and can be answered through the use of machine learning linear regression.\n",
    "The question is:\n",
    "- How many deaths expected in the next week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEATHS_BY_DATE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets, excluding the last 7 days for testing\n",
    "train_data = DEATHS_BY_DATE[:-7]\n",
    "test_data = DEATHS_BY_DATE[-7:]\n",
    "X_train = train_data.index.map(pd.Timestamp.toordinal).values.reshape(-1, 1)\n",
    "y_train = train_data.values\n",
    "X_test = test_data.index.map(pd.Timestamp.toordinal).values.reshape(-1, 1)\n",
    "y_test = test_data.values\n",
    "# Create and fit the model on the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "# Combine actual and predicted values into a DataFrame for comparison\n",
    "test_dates = test_data.index\n",
    "predicted_vs_actual = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predictions.round(0)\n",
    "}, index=test_dates)\n",
    "predicted_vs_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot actual values\n",
    "plt.plot_date(test_data.index, y_test, linestyle='solid', marker=None, label='Actual')\n",
    "# Plot predicted values\n",
    "plt.plot_date(test_data.index, predictions.round(0), linestyle='solid', marker=None, label='Predicted')\n",
    "# Adding the legend\n",
    "plt.legend()\n",
    "# Adding a grid\n",
    "plt.grid(True)\n",
    "# Rotate and align the tick labels so they look better\n",
    "plt.gcf().autofmt_xdate()\n",
    "# Set title and labels\n",
    "plt.title('Actual vs Predicted Deaths')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Deaths')\n",
    "# Show plot with tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_,model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "- In my opinion, if we are going to use the Linear Regression, we can choose to start from a point that looks like a line start. It may be after April."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEATHS_BY_DATE = DEATHS_BY_DATE[DEATHS_BY_DATE.index>='2020-04-01'] #Data after April 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets, excluding the last 7 days for testing\n",
    "train_data = DEATHS_BY_DATE[:-7]\n",
    "test_data = DEATHS_BY_DATE[-7:]\n",
    "X_train = train_data.index.map(pd.Timestamp.toordinal).values.reshape(-1, 1)\n",
    "y_train = train_data.values\n",
    "X_test = test_data.index.map(pd.Timestamp.toordinal).values.reshape(-1, 1)\n",
    "y_test = test_data.values\n",
    "# Create and fit the model on the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "# Combine actual and predicted values into a DataFrame for comparison\n",
    "test_dates = test_data.index\n",
    "predicted_vs_actual = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predictions.round(0)\n",
    "}, index=test_dates)\n",
    "predicted_vs_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot actual values\n",
    "plt.plot_date(test_data.index, y_test, linestyle='solid', marker=None, label='Actual')\n",
    "# Plot predicted values\n",
    "plt.plot_date(test_data.index, predictions.round(0), linestyle='solid', marker=None, label='Predicted')\n",
    "# Adding the legend\n",
    "plt.legend()\n",
    "# Adding a grid\n",
    "plt.grid(True)\n",
    "# Rotate and align the tick labels so they look better\n",
    "plt.gcf().autofmt_xdate()\n",
    "# Set title and labels\n",
    "plt.title('Actual vs Predicted Deaths')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Deaths')\n",
    "# Show plot with tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Better accuracy achieved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_,model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Futuristic Question 2: Can we infere about deaths using other features rather than date? Example if I have the number of active cases, can I infere about deaths?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we learn how to find correlation between features to be then used in linear regression modelling.\n",
    "- The groupby is used to find the sum of Deaths,Active,Recovered,Confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG = df.groupby('Date',as_index=False)[['Deaths','Active','Recovered','Confirmed']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correlation means the relation between one feature and another. It measures how strong the relationship between two variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'corr.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sb.heatmap(AGG.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG['Date'] = AGG['Date'].map(pd.Timestamp.toordinal).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets, excluding the last 7 days for testing\n",
    "train_data = AGG[:-7]\n",
    "test_data = AGG[-7:]\n",
    "X_train = train_data.values[:,[0,2,3,4]]\n",
    "y_train = train_data.values[:,1] #deaths\n",
    "X_test = test_data.values[:,[0,2,3,4]]\n",
    "y_test = test_data.values[:,1]\n",
    "# Create and fit the model on the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "# Combine actual and predicted values into a DataFrame for comparison\n",
    "test_dates = test_data.index\n",
    "predicted_vs_actual = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predictions.round(0)\n",
    "}, index=test_dates)\n",
    "predicted_vs_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_,model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Work: Build your own model to answer the following question: - How many recovery are expected to occur in the next 10 days? \n",
    "- Note: You are allowed to use multiple features as input, you can use the correlation matrix to find relation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets, excluding the last 7 days for testing\n",
    "train_data = AGG[:-10]\n",
    "test_data = AGG[-10:]\n",
    "X_train = train_data.values[:,[0,1,2,4]]\n",
    "y_train = train_data.values[:,3] #deaths\n",
    "X_test = test_data.values[:,[0,1,2,4]]\n",
    "y_test = test_data.values[:,3]\n",
    "# Create and fit the model on the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "# Combine actual and predicted values into a DataFrame for comparison\n",
    "test_dates = test_data.index\n",
    "predicted_vs_actual = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predictions.round(0)\n",
    "}, index=test_dates)\n",
    "predicted_vs_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem: New dataset of detailed Covid-19 individual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical and machine learning technique used for classification tasks, particularly binary classification. It is used to predict the probability of a target variable (dependent variable) based on one or more predictor variables (independent variables). Despite its name, logistic regression is used for classification rather than regression tasks.\n",
    "\n",
    "### Input:\n",
    "- **Predictor Variables (Features)**: The input to logistic regression can be one or more predictor variables (X) that can be either continuous or categorical. These variables are used to predict the outcome of the target variable.\n",
    "- **Target Variable (Label)**: The target variable (Y) in logistic regression is binary, meaning it can take on only two possible outcomes. For example, \"1\" for success and \"0\" for failure, or \"yes\" for presence and \"no\" for absence of a condition.\n",
    "\n",
    "### Output:\n",
    "- **Probability**: The output of logistic regression is a probability that ranges between 0 and 1. This probability represents the likelihood of the target variable being in one of the two classes (e.g., 1 or 0, yes or no). Based on a threshold (commonly 0.5), the model assigns each observation to one of the two classes.\n",
    "\n",
    "### How It Works:\n",
    "1. **Logistic Function (Sigmoid Function)**: Logistic regression uses the logistic (sigmoid) function to model the probability that each input belongs to a particular category. The sigmoid function outputs a value between 0 and 1, which is interpreted as a probability. The function is defined as:\n",
    "$$ P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\dots + \\beta_kX_k)}} $$\n",
    "\n",
    "- **Where:**\n",
    "\n",
    "    - **P(Y=1)**: is the probability that the target variable Y is in class 1.\n",
    "    - **e**: is the base of the natural logarithm.\n",
    "    - **β0, β1, ..., βk**: are the coefficients of the model.\n",
    "    - **X1, ..., Xk**: are the predictor variables.\n",
    "\n",
    "2. **Estimating Coefficients**: The coefficients (\\(\\beta\\)) of the logistic regression model are estimated using maximum likelihood estimation (MLE). This method finds the set of coefficients that maximizes the likelihood of the observed set of responses with the given set of predictor variables.\n",
    "\n",
    "3. **Decision Boundary**: Logistic regression models the decision boundary between the classes. The decision boundary can be linear or nonlinear, depending on the logistic regression variant used (e.g., linear logistic regression or logistic regression with polynomial features).\n",
    "\n",
    "4. **Classification and Probability Estimation**: Once the model is trained (i.e., the coefficients are estimated), it can be used to predict the probability of new observations belonging to each class. If the predicted probability is greater than or equal to 0.5, the observation is classified into class 1; otherwise, it is classified into class 0.\n",
    "\n",
    "Logistic regression is widely used in various fields such as medicine, finance, and social sciences for binary classification problems because of its simplicity, interpretability, and effectiveness in situations where the relationship between the predictor variables and the log odds of the outcome is approximately linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data acquired:\n",
    "    - Attribute\n",
    "    - Description\n",
    "    - Clean data\n",
    "- Problem statement\n",
    "    - Assessing the data\n",
    "    - Problem formulation [Classification or Regression]\n",
    "- Modelling\n",
    "    - Model initiation \n",
    "        - from sklearn.linear_model import LogisticRegression\n",
    "        - model = LogisiticRegression()\n",
    "    - Training and Testing split for the data\n",
    "        - from sklearn.model_selection import train_test_split\n",
    "        - X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    - Model fitting\n",
    "        - model.fit(X_train,y_train)\n",
    "    - Model predict\n",
    "        - y_pred = model.precit(X_test)\n",
    "    - Model evaluate\n",
    "        - Comparison between the y_pred and actual y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling [Data Preparation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Covid-19 Classification Data**\n",
    "\n",
    "Contains a vast number of anonymized patient-related information including pre-conditions. The raw dataset consists of 21 different features and 1,048,576 unique patients. In the Boolean features, 1 means \"yes\" and 2 means \"no\". values as 97 and 99 are missing data.\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/meirnizri/covid19-dataset?resource=download\n",
    "\n",
    "**Attributes** :\n",
    " \n",
    "content\n",
    "The dataset was provided by the Mexican government (link). This dataset contains an enormous number of anonymized patient-related information including pre-conditions. The raw dataset consists of 21 unique features and 1,048,576 unique patients. In the Boolean features, 1 means \"yes\" and 2 means \"no\". values as 97 and 99 are missing data.\n",
    "\n",
    "- sex: 1 for female and 2 for male.\n",
    "- age: of the patient.\n",
    "- classification: covid test findings. Values 1-3 mean that the patient was diagnosed with covid in different degrees. 4 or higher means that the patient is not a carrier of covid or that the test is inconclusive.\n",
    "- patient type: type of care the patient received in the unit. 1 for returned home and 2 for hospitalization.\n",
    "- pneumonia: whether the patient already have air sacs inflammation or not.\n",
    "- pregnancy: whether the patient is pregnant or not.\n",
    "- diabetes: whether the patient has diabetes or not.\n",
    "- copd: Indicates whether the patient has Chronic obstructive pulmonary disease or not.\n",
    "- asthma: whether the patient has asthma or not.\n",
    "- inmsupr: whether the patient is immunosuppressed or not.\n",
    "- hypertension: whether the patient has hypertension or not.\n",
    "- cardiovascular: whether the patient has heart or blood vessels related disease.\n",
    "- renal chronic: whether the patient has chronic renal disease or not.\n",
    "- other disease: whether the patient has other disease or not.\n",
    "- obesity: whether the patient is obese or not.\n",
    "- tobacco: whether the patient is a tobacco user.\n",
    "- usmr: Indicates whether the patient treated medical units of the first, second or third level.\n",
    "- medical unit: type of institution of the National Health System that provided the care.\n",
    "- intubed: whether the patient was connected to the ventilator.\n",
    "- icu: Indicates whether the patient had been admitted to an Intensive Care Unit.\n",
    "- date died: If the patient died indicate the date of death, and 9999-99-99 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('classification_covid.zip',compression='zip')\n",
    "pd.set_option('display.max_columns',None,'display.max_rows',None,'display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(500000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   USMER                 500000 non-null  int64 \n",
      " 1   MEDICAL_UNIT          500000 non-null  int64 \n",
      " 2   SEX                   500000 non-null  int64 \n",
      " 3   PATIENT_TYPE          500000 non-null  int64 \n",
      " 4   DATE_DIED             500000 non-null  object\n",
      " 5   INTUBED               500000 non-null  int64 \n",
      " 6   PNEUMONIA             500000 non-null  int64 \n",
      " 7   AGE                   500000 non-null  int64 \n",
      " 8   PREGNANT              500000 non-null  int64 \n",
      " 9   DIABETES              500000 non-null  int64 \n",
      " 10  COPD                  500000 non-null  int64 \n",
      " 11  ASTHMA                500000 non-null  int64 \n",
      " 12  INMSUPR               500000 non-null  int64 \n",
      " 13  HIPERTENSION          500000 non-null  int64 \n",
      " 14  OTHER_DISEASE         500000 non-null  int64 \n",
      " 15  CARDIOVASCULAR        500000 non-null  int64 \n",
      " 16  OBESITY               500000 non-null  int64 \n",
      " 17  RENAL_CHRONIC         500000 non-null  int64 \n",
      " 18  TOBACCO               500000 non-null  int64 \n",
      " 19  CLASIFFICATION_FINAL  500000 non-null  int64 \n",
      " 20  ICU                   500000 non-null  int64 \n",
      "dtypes: int64(20), object(1)\n",
      "memory usage: 80.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Missing Data**:\n",
    "- First, counting the proportion of missing data (the ones having 97,98,99 codes)\n",
    "- Second, look by manual inspection for the date died\n",
    "- Third, choose which feature to be dropped then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USMER': 0.0,\n",
       " 'MEDICAL_UNIT': 0.0,\n",
       " 'SEX': 0.0,\n",
       " 'PATIENT_TYPE': 0.0,\n",
       " 'DATE_DIED': 0.926798,\n",
       " 'INTUBED': 0.81711,\n",
       " 'PNEUMONIA': 0.015264,\n",
       " 'AGE': 0.00033,\n",
       " 'PREGNANT': 0.502696,\n",
       " 'DIABETES': 0.003106,\n",
       " 'COPD': 0.002782,\n",
       " 'ASTHMA': 0.00274,\n",
       " 'INMSUPR': 0.003146,\n",
       " 'HIPERTENSION': 0.002872,\n",
       " 'OTHER_DISEASE': 0.004692,\n",
       " 'CARDIOVASCULAR': 0.002868,\n",
       " 'OBESITY': 0.002852,\n",
       " 'RENAL_CHRONIC': 0.00278,\n",
       " 'TOBACCO': 0.003012,\n",
       " 'CLASIFFICATION_FINAL': 0.0,\n",
       " 'ICU': 0.81726}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for column in data.columns:    \n",
    "    # Count the occurrences of special codes (97, 98, 99) in these columns\n",
    "    special_code_counts = {column: data[column].value_counts().loc[lambda x: x.index.isin([97, 98, 99,'9999-99-99'])]\\\n",
    "        .sum() for column in data.columns}\n",
    "    # Total count for perspective\n",
    "    total_counts = {column: data[column].count() for column in data.columns}\n",
    "    # Find total special code\n",
    "    special_code_proportions = {column: special_code_counts[column] / total_counts[column] for column in data.columns}\n",
    "special_code_proportions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next the optimum approach is to exclude the features with high missing values.\n",
    "- To perform this, it is better to do it dynamically (removing based on a filter not by manual inspection)\n",
    "- For me I have chosen the features with high 0.2 to be dropped (along side with the Y which is the target column) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USMER': 0.0,\n",
       " 'MEDICAL_UNIT': 0.0,\n",
       " 'SEX': 0.0,\n",
       " 'PATIENT_TYPE': 0.0,\n",
       " 'PNEUMONIA': 0.015264,\n",
       " 'AGE': 0.00033,\n",
       " 'DIABETES': 0.003106,\n",
       " 'COPD': 0.002782,\n",
       " 'ASTHMA': 0.00274,\n",
       " 'INMSUPR': 0.003146,\n",
       " 'HIPERTENSION': 0.002872,\n",
       " 'OTHER_DISEASE': 0.004692,\n",
       " 'CARDIOVASCULAR': 0.002868,\n",
       " 'OBESITY': 0.002852,\n",
       " 'RENAL_CHRONIC': 0.00278,\n",
       " 'TOBACCO': 0.003012}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dynamic_filtered_features = {k: v for k, v in special_code_proportions.items() if v <= 0.1 and k not in ['CLASIFFICATION_FINAL']}\n",
    "dynamic_filtered_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Classification final should be encoded to either 0 or 1 by checking the description you will find that:\n",
    "    - People with 0 to 3 are diagnosed as covid-19\n",
    "    - People with 4 or more are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASIFFICATION_FINAL\n",
       "7    237856\n",
       "3    181969\n",
       "6     61200\n",
       "5     12505\n",
       "1      4117\n",
       "4      1483\n",
       "2       870\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['CLASIFFICATION_FINAL'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CLASIFFICATION_FINAL'] = data['CLASIFFICATION_FINAL'].map({1: 'POS', 2: 'POS', 3: 'POS', 4:'NEG', 5:'NEG', 6:'NEG', 7:'NEG'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Metwalli\\AppData\\Local\\Temp\\ipykernel_37060\\3399123075.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mask['CLASIFFICATION_FINAL'] = data['CLASIFFICATION_FINAL'] # Add the target column back\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(488543, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update feature selection based on the analysis above\n",
    "updated_features = list(dynamic_filtered_features.keys())\n",
    "mask = data[updated_features] # Select only the updated features from the dataset\n",
    "mask['CLASIFFICATION_FINAL'] = data['CLASIFFICATION_FINAL'] # Add the target column back\n",
    "for column in mask.columns:\n",
    "    if column != 'CLASIFFICATION_FINAL':\n",
    "        mask = mask[(mask[column]<96)] # Excluding the 97,98,99 missing data from data\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update feature selection based on the analysis above\n",
    "X_updated = mask.drop('CLASIFFICATION_FINAL',axis=1)\n",
    "Y = mask['CLASIFFICATION_FINAL']\n",
    "# Split the updated dataset into training and testing sets\n",
    "X_train_updated, X_test_updated, Y_train, Y_test = train_test_split(X_updated, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.67      0.90      0.77     12195\n",
      "         POS       0.61      0.25      0.35      7338\n",
      "\n",
      "    accuracy                           0.66     19533\n",
      "   macro avg       0.64      0.58      0.56     19533\n",
      "weighted avg       0.65      0.66      0.61     19533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the logistic regression model on the updated dataset\n",
    "model_updated = LogisticRegression(n_jobs=-1)\n",
    "model_updated.fit(X_train_updated, Y_train)\n",
    "\n",
    "# Updated predictions\n",
    "Y_pred_updated = model_updated.predict(X_test_updated)\n",
    "# Evaluate the updated model\n",
    "accuracy_updated = accuracy_score(Y_test, Y_pred_updated)\n",
    "report_updated = classification_report(y_true=Y_test, y_pred=Y_pred_updated) # Actual Y and the predicted (Y_hat)\n",
    "\n",
    "print(report_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model just achieved 66% accuracy due to one or more of the following reasons:\n",
    "    - The data balance can be enhanced by balancing both positive and negative classes. \n",
    "    - Some data used in this classification may be irrelevant. (Will discover later in Regularization)\n",
    "    - The sample data are not ideal since we do not have an equiprobable number of output.\n",
    "    - The logistic regression may not be the best practice since it has the following disadvantages:\n",
    "        - Logistic regression only works well for cases where the dataset is linearly separable because:\n",
    "            - Logistic regression assumes linearity between the predicted (dependent) variable and the predictor (independent) variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the classification model: [CONFUSION MATRIX] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Confusion Matrix is a performance measurement tool for machine learning classification. It is a table with two dimensions (\"Actual\" and \"Predicted\"), and it allows us to visualize the performance of an algorithm. Here's a breakdown of its components:\n",
    "\n",
    "<img src='conf_matrix.png' width='400' height='350' style='display: block; margin: auto;'>\n",
    "\n",
    "## Structure\n",
    "- **True Positives (TP)**: The cases in which the actual class of the data point was 1 (True) and the predicted is also 1 (True).\n",
    "- **True Negatives (TN)**: The cases in which the actual class of the data point was 0 (False) and the predicted is also 0 (False).\n",
    "- **False Positives (FP)**: The cases in which the actual class of the data point was 0 (False) and the predicted is 1 (True), also known as a \"Type I error\". The model incorrectly predicted the positive class.\n",
    "- **False Negatives (FN)**: The cases in which the actual class of the data point was 1 (True) and the predicted is 0 (False), also known as a \"Type II error\". The model incorrectly predicted the negative class.\n",
    "\n",
    "## Metrics Derived\n",
    "- **Accuracy**: Overall, how often is the classifier correct? `(TP + TN) / (TP + TN + FP + FN)`\n",
    "- **Precision** (or Positive Predictive Value): When it predicts yes, how often is it correct? `TP / (TP + FP)`\n",
    "- **Recall** (or Sensitivity or True Positive Rate): How often it predicts yes, when it's actually yes? `TP / (TP + FN)`\n",
    "- **F1 Score**: A weighted average of Precision and Recall. `2 * (Precision * Recall) / (Precision + Recall)`\n",
    "- **Specificity** (or True Negative Rate): How often it predicts no, when it's actually no? `TN / (TN + FP)`\n",
    "- **False Positive Rate**: When it's actually no, how often does it predict yes? `FP / (FP + TN)`\n",
    "- **Negative Predictive Value**: When it predicts no, how often is it correct? `TN / (TN + FN)`\n",
    "- **False Discovery Rate**: When it predicts yes, how often is it wrong? `FP / (FP + TP)`\n",
    "\n",
    "## Importance\n",
    "Understanding these metrics and the structure of a confusion matrix is crucial for interpreting the performance of classification models, allowing for the identification of areas for improvement, and balancing precision and recall according to the business requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating the confusion matrix\n",
    "LABELS = ['NEG','POS']\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred_updated, labels=LABELS)\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(7,4))\n",
    "sb.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues',xticklabels=LABELS, yticklabels=LABELS)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a technique applied as a part of data preprocessing to standardize the range of features of data. It is also known as feature scaling. In this lesson, we will explore two common normalization techniques: Min-Max Scaling and Z-Score Standardization.\n",
    "\n",
    "## 1. Min-Max Scaling\n",
    "\n",
    "Min-Max Scaling (or Min-Max Normalization) is one of the simplest methods and scales the feature to a fixed range, typically 0 to 1, or -1 to 1. The formula for calculating Min-Max Scaling is:\n",
    "\n",
    "$$ \\text{X}_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $$\n",
    "\n",
    "### Example\n",
    "If the range of a feature is from 20 to 80, and the actual value of an observation is 40, it is normalized as follows:\n",
    "$$ \\text{X}_{\\text{norm}} = \\frac{40 - 20}{80 - 20} = 0.333 $$\n",
    "\n",
    "## 2. Z-Score Standardization\n",
    "\n",
    "Z-Score Standardization (or just Standardization) is another common technique that involves rescaling the features so that they’ll have the properties of a standard normal distribution with \\( \\mu = 0 \\) and \\( \\sigma = 1 \\), where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation from the mean.\n",
    "\n",
    "The formula for Z-Score Standardization is:\n",
    "\n",
    "$$ Z = \\frac{(X - \\mu)}{\\sigma} $$\n",
    "\n",
    "### Example\n",
    "If the mean of a feature is 50 and the standard deviation is 10, and the actual value of an observation is 60, it is standardized as follows:\n",
    "$$ Z = \\frac{(60 - 50)}{10} = 1.0 $$\n",
    "\n",
    "These normalization methods are essential, especially in algorithms that are sensitive to the magnitude of values and require normalization as a preprocessing step, such as k-nearest neighbors (KNN) and gradient descent based algorithms like linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update feature selection based on the analysis above\n",
    "X_updated = mask.drop('CLASIFFICATION_FINAL', axis=1)\n",
    "Y = mask['CLASIFFICATION_FINAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the updated dataset into training and testing sets\n",
    "X_train_updated, X_test_updated, Y_train, Y_test = train_test_split(X_updated, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_updated = scaler.fit_transform(X_train_updated)\n",
    "\n",
    "# Apply the same transformation to the test data\n",
    "X_test_updated = scaler.transform(X_test_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "- Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. \n",
    "- The main goal of cross-validation is: \n",
    "    - to flag problems like overfitting or selection bias\n",
    "    - to give an insight on how the model will generalize to an independent dataset.\n",
    "\n",
    "**Why Use Cross-Validation?**\n",
    "- Model Validation: It helps in assessing how well the model will generalize to an independent dataset.\n",
    "- Tuning Parameters: It aids in selecting the best parameters for the machine learning model.\n",
    "- Feature Selection: It can help in determining which features are valuable for the model’s prediction.\n",
    "\n",
    "**Types of Cross-Validation**\n",
    "- K-Fold Cross-Validation: The dataset is divided into K equal subsets. Each time, one of the K subsets is used as the test set and the other K-1 subsets are put together to form a training set. The process is repeated K times, with each of the K subsets used exactly once as the test data.\n",
    "- Stratified K-Fold Cross-Validation: Similar to K-fold cross-validation, but in this method, the folds are made by preserving the percentage of samples for each class.\n",
    "- Leave-One-Out Cross-Validation (LOOCV): A special case of k-fold cross-validation where K equals the number of data points in the dataset. Each learning set is created by taking all the samples except one, the test data set being the sample left out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'cross_validation.png' width =\"600\" height =\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'inside_cv.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.66061071 0.66039023 0.6619469  0.66233074 0.65505917]\n",
      "Mean CV Accuracy: 0.6600675525718602\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.66      0.91      0.77     36368\n",
      "         POS       0.63      0.25      0.36     22252\n",
      "\n",
      "    accuracy                           0.66     58620\n",
      "   macro avg       0.65      0.58      0.56     58620\n",
      "weighted avg       0.65      0.66      0.61     58620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the logistic regression model\n",
    "model_updated = LogisticRegression(n_jobs=-1)\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform K-Fold CV to evaluate model\n",
    "scores = cross_val_score(model_updated, X_train_updated, Y_train, cv=kf, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy Scores: {scores}')\n",
    "print(f'Mean CV Accuracy: {scores.mean()}')\n",
    "\n",
    "# Since cross_val_score only evaluates performance, you still need to train your model on the entire training set\n",
    "model_updated.fit(X_train_updated, Y_train)\n",
    "# Make predictions on the test set\n",
    "Y_pred_updated = model_updated.predict(X_test_updated)\n",
    "# Evaluate the model on the test set\n",
    "accuracy_updated = accuracy_score(Y_test, Y_pred_updated)\n",
    "report_updated = classification_report(Y_test, Y_pred_updated)  # Actual Y and the predicted (Y_hat)\n",
    "print(report_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Initialize the logistic regression model\n",
    "#model_updated = LogisticRegression(n_jobs=-1)\n",
    "model_updated = KNeighborsClassifier(n_neighbors = 3)\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Perform K-Fold CV to evaluate model\n",
    "scores = cross_val_score(model_updated, X_train_updated, Y_train, cv=kf, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy Scores: {scores}')\n",
    "print(f'Mean CV Accuracy: {scores.mean()}')\n",
    "\n",
    "# Since cross_val_score only evaluates performance, you still need to train your model on the entire training set\n",
    "model_updated.fit(X_train_updated, Y_train)\n",
    "# Make predictions on the test set\n",
    "Y_pred_updated = model_updated.predict(X_test_updated)\n",
    "# Evaluate the model on the test set\n",
    "accuracy_updated = accuracy_score(Y_test, Y_pred_updated)\n",
    "report_updated = classification_report(Y_test, Y_pred_updated)  # Actual Y and the predicted (Y_hat)\n",
    "print(report_updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion and Findings**\n",
    "- After implementing the cross-validation technique of k-folds on k=5, we can conclude that:\n",
    "    - Cross-Validation accuracy scores across the different folds of cross-validation are relatively consistent, with scores ranging from approximately 0.654 to 0.660. \n",
    "    - This indicates that the model is fairly stable and is generalizing well across different subsets of the data.\n",
    "\n",
    "- **Notes**\n",
    "    - Underfitting is found when the cross-validation scores are uniformly low\n",
    "    - while overfitting is detected when the scores vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Regularization\n",
    "\n",
    "- Regularization is a technique used in machine learning and statistics to prevent overfitting by penalizing large coefficients in a model. \n",
    "- Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. \n",
    "- Regularization techniques add a penalty on the size of the coefficients to the loss function. \n",
    "- Common types of regularization include L1 regularization (Lasso), L2 regularization (Ridge), and Elastic Net, which is a combination of L1 and L2 regularization.\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "Regularization is a fundamental technique in machine learning designed to prevent overfitting and improve model generalization. This is achieved by adding a penalty term to the loss function, which discourages overly complex models by penalizing large coefficients. The general form of a regularized loss function is:\n",
    "\n",
    "$$ \\text{Loss}_{\\text{regularized}} = \\text{Loss}_{\\text{original}} + \\lambda \\times \\text{Penalty} $$\n",
    "\n",
    "Here, $\\text{Loss}_{\\text{original}}$ represents the original loss function, such as Mean Squared Error (MSE) for regression tasks. The parameter $\\lambda \\geq 0$ is known as the regularization strength; higher values of $\\lambda$ apply a stronger penalty, thus encouraging simpler models. The $\\text{Penalty}$ term is responsible for imposing costs on the size of coefficients, and its form varies depending on the type of regularization applied.\n",
    "\n",
    "### L1 Regularization (Lasso)\n",
    "\n",
    "L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator), imposes a penalty equal to the absolute value of the magnitude of coefficients. This can be mathematically represented as:\n",
    "\n",
    "$$ \\text{Penalty}_{L1} = \\sum_{i=1}^{n} |w_i| $$\n",
    "\n",
    "where $w_i$ are the model coefficients. L1 regularization tends to produce sparse models, where a subset of coefficients can become exactly zero. This property makes Lasso regularization particularly useful for feature selection in models with high-dimensional data.\n",
    "\n",
    "### L2 Regularization (Ridge)\n",
    "\n",
    "L2 regularization, known as Ridge regression, applies a penalty equal to the square of the magnitude of coefficients:\n",
    "\n",
    "$$ \\text{Penalty}_{L2} = \\sum_{i=1}^{n} w_i^2 $$\n",
    "\n",
    "This form of regularization tends to distribute the penalty among all coefficients, pushing them closer to zero but rarely to zero. Unlike L1 regularization, L2 does not naturally result in feature selection but is effective in handling multicollinearity (when independent variables are highly correlated) by distributing the coefficient values across similar features.\n",
    "\n",
    "### Key Differences between L1 and L2 Regularization\n",
    "\n",
    "The core difference between L1 and L2 regularization lies in their penalty terms and the resulting impact on the model coefficients:\n",
    "\n",
    "- **Sparsity**: L1 regularization can produce sparse models, making it a natural choice for feature selection. L2 regularization does not induce sparsity but can shrink the coefficients more uniformly.\n",
    "- **Solution Path**: The solution path of L1 regularization is piecewise linear, which can lead to models where some coefficients are exactly zero if the regularization strength is sufficiently high. L2 regularization ensures coefficients approach zero asymptotically.\n",
    "- **Robustness and Stability**: L1 is more robust to outliers than L2 because it penalizes the absolute value of coefficients, leading to smaller and more robust coefficients. However, L2 is more stable in terms of the numerical solution, especially when dealing with multicollinearity.\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "Elastic Net is a hybrid regularization technique that combines the penalties of L1 and L2 regularization:\n",
    "\n",
    "$$ \\text{Penalty}_{\\text{ElasticNet}} = \\rho \\sum_{i=1}^{n} |w_i| + (1 - \\rho) \\sum_{i=1}^{n} w_i^2 $$\n",
    "\n",
    "Here, $\\rho$ is the mixing parameter that controls the balance between L1 and L2 regularization penalties. Elastic Net is particularly useful when there are multiple features correlated with each other. It combines the feature selection capability of L1 with the regularization strength of L2, making it a versatile choice for various modeling scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha using built-in ElasticNetCV(): 0.002471\n",
      "Best score using built-in ElasticNetCV(): 0.057068\n",
      "ElasticNetCV() picked 12 variables and eliminated the other 4 variables\n",
      "PNEUMONIA        -0.136976\n",
      "OBESITY          -0.057718\n",
      "DIABETES         -0.014472\n",
      "MEDICAL_UNIT     -0.003682\n",
      "COPD              0.000000\n",
      "ASTHMA            0.000000\n",
      "HIPERTENSION     -0.000000\n",
      "RENAL_CHRONIC     0.000000\n",
      "AGE               0.002679\n",
      "USMER             0.005431\n",
      "INMSUPR           0.006267\n",
      "CARDIOVASCULAR    0.008793\n",
      "OTHER_DISEASE     0.022971\n",
      "SEX               0.031848\n",
      "TOBACCO           0.039423\n",
      "PATIENT_TYPE      0.096794\n",
      "dtype: float64\n",
      "Best alpha using built-in RidgeCV(): 10.000000\n",
      "Best score using built-in RidgeCV(): 0.059231\n",
      "RidgeCV() picked 16 variables and eliminated the other 0 variables\n",
      "PNEUMONIA        -0.147046\n",
      "OBESITY          -0.069028\n",
      "DIABETES         -0.030460\n",
      "HIPERTENSION     -0.005046\n",
      "MEDICAL_UNIT     -0.003955\n",
      "AGE               0.002610\n",
      "USMER             0.012516\n",
      "ASTHMA            0.022855\n",
      "SEX               0.036296\n",
      "TOBACCO           0.055242\n",
      "OTHER_DISEASE     0.062187\n",
      "CARDIOVASCULAR    0.063525\n",
      "RENAL_CHRONIC     0.064092\n",
      "COPD              0.080856\n",
      "INMSUPR           0.084719\n",
      "PATIENT_TYPE      0.107682\n",
      "dtype: float64\n",
      "Best alpha using built-in LassoCV(): 0.001236\n",
      "Best score using built-in LassoCV(): 0.057129\n",
      "LassoCV() picked 12 variables and eliminated the other 4 variables\n",
      "PNEUMONIA        -0.138543\n",
      "OBESITY          -0.058277\n",
      "DIABETES         -0.014418\n",
      "MEDICAL_UNIT     -0.003676\n",
      "COPD              0.000000\n",
      "ASTHMA            0.000000\n",
      "HIPERTENSION     -0.000000\n",
      "RENAL_CHRONIC     0.000000\n",
      "AGE               0.002670\n",
      "USMER             0.005616\n",
      "INMSUPR           0.006858\n",
      "CARDIOVASCULAR    0.009473\n",
      "OTHER_DISEASE     0.024132\n",
      "SEX               0.031962\n",
      "TOBACCO           0.040155\n",
      "PATIENT_TYPE      0.096918\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def APPLY_REGULARIZATION(reg):\n",
    "    # Fit the regularization model\n",
    "    reg.fit(X_train_updated, Y_train.map({'NEG':0,'POS':1}))\n",
    "    # Find the alpha and the score\n",
    "    print(\"Best alpha using built-in {}: %f\".format(str(reg).split('.')[-1]) % reg.alpha_)\n",
    "    print(\"Best score using built-in {}: %f\".format(str(reg).split('.')[-1]) %reg.score(X_train_updated,Y_train.map({'NEG':0,'POS':1})))\n",
    "    coef = pd.Series(reg.coef_, index = X_updated.columns)\n",
    "    print(\"{} picked \".format(str(reg).split('.')[-1]) + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "    imp_coef = coef.sort_values()\n",
    "    print(imp_coef)\n",
    "# Initialize regularization and apply\n",
    "APPLY_REGULARIZATION(ElasticNetCV()) # L1 + L2 Type\n",
    "APPLY_REGULARIZATION(RidgeCV()) # L2 Type\n",
    "APPLY_REGULARIZATION(LassoCV()) # L1 Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "- After applying the regularization technique that has the ability to do feature selection such as (LASSO and ELASTICNET) L1s, based on the coefficient of each feature we conclude that:\n",
    "    - The ASTHMA and the HIPERTENSION can be dropped since their coefficients are 0.\n",
    "    - Next step is to re-apply the algorithms of ML after dropping the RENAL_CHRONIC, COPD, ASTHMA and the HIPERTENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.66      0.91      0.77     60475\n",
      "         POS       0.63      0.25      0.36     37234\n",
      "\n",
      "    accuracy                           0.66     97709\n",
      "   macro avg       0.65      0.58      0.56     97709\n",
      "weighted avg       0.65      0.66      0.61     97709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_drop = ['RENAL_CHRONIC','COPD','ASTHMA','HIPERTENSION']\n",
    "# Split the updated dataset into training and testing sets\n",
    "X_train_updated, X_test_updated, Y_train, Y_test = train_test_split(X_updated.drop(to_drop,axis=1), Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_updated = scaler.fit_transform(X_train_updated)\n",
    "\n",
    "# Apply the same transformation to the test data\n",
    "X_test_updated = scaler.transform(X_test_updated)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model_updated = LogisticRegression(n_jobs=-1)\n",
    "model_updated.fit(X_train_updated, Y_train)\n",
    "# Make predictions on the test set\n",
    "Y_pred_updated = model_updated.predict(X_test_updated)\n",
    "# Evaluate the model on the test set\n",
    "accuracy_updated = accuracy_score(Y_test, Y_pred_updated)\n",
    "report_updated = classification_report(Y_test, Y_pred_updated)  # Actual Y and the predicted (Y_hat)\n",
    "print(report_updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adropping the ASTHMA and the HIPERTENSION, we can notice that the accuracy, precision and recall are unchanged.\n",
    "- Threfore, they have no effect on classification as the regularization showed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple, yet powerful machine learning algorithm used for both classification and regression tasks. Unlike logistic regression, which is a predictive modeling technique based on the assumption of a mathematical relationship between input variables and the probability of the target variable, KNN works by finding the most similar historical data points to the new data point and predicting the outcome based on the outcomes of these neighbors.\n",
    "\n",
    "**Input:**\n",
    "- Predictor Variables (Features): KNN uses predictor variables (X) similar to logistic regression. These variables describe the characteristics of the data points. The algorithm uses these features to find the nearest neighbors of a new data point.\n",
    "- Target Variable (Label): The target variable (Y) in KNN can be either categorical for classification tasks or continuous for regression tasks. For classification, the algorithm assigns a class to the new data point based on the majority vote of its nearest neighbors.\n",
    "\n",
    "**Output:**\n",
    "- Class Membership (for classification): In classification tasks, KNN outputs the class with the majority vote among the K nearest neighbors of the new data point.\n",
    "- Value Prediction (for regression): In regression tasks, KNN predicts the value for the new data point based on the average (or another aggregate measure) of the values of its K nearest neighbors.\n",
    "\n",
    "**How It Works:**\n",
    "- Choosing K: The first step in KNN is to choose the number of neighbors (K). This is a crucial parameter that influences the performance of the algorithm. A small value of K can make the algorithm sensitive to noise in the data, while a large value makes it computationally expensive and may lead to underfitting.\n",
    "- Distance Measure: To find the nearest neighbors, KNN calculates the distance between data points using a distance measure such as Euclidean, Manhattan, or Minkowski distance. The choice of distance measure can affect the performance of the algorithm.\n",
    "- Finding Nearest Neighbors: For each new data point, the algorithm searches the training dataset to find the K nearest neighbors based on the distance measure.\n",
    "\n",
    "- Majority Vote or Averaging:\n",
    "\n",
    "    - Classification: For classification tasks, KNN assigns the class based on the majority class among the K nearest neighbors. Ties can be broken randomly or by reducing K until a clear majority is found.\n",
    "    - Regression: For regression tasks, KNN predicts the value by averaging (or taking another aggregate measure) the values of the K nearest neighbors.\n",
    "- Prediction: The new data point is assigned the computed class (for classification) or value (for regression) based on its nearest neighbors.\n",
    "\n",
    "KNN is widely appreciated for its simplicity, ease of understanding, and effectiveness in many scenarios. However, it can be computationally expensive as it requires storing the entire dataset and calculating distances between data points for each prediction. Additionally, the performance of KNN can be significantly impacted by the choice of K and the distance measure used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Preprocessing before applying KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First let's do better data preprocessing after the Cross-Validation and Regularization\n",
    "    - Removing irrelevant data\n",
    "        - I found that \n",
    "            - USMER and MEDICAL_UNIT are irrelevant to classification task so we will remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN\n",
    "model_updated = KNN(10)\n",
    "model_updated.fit(X_train_updated, Y_train)\n",
    "# Make predictions on the test set\n",
    "Y_pred_updated = model_updated.predict(X_test_updated)\n",
    "# Evaluate the model on the test set\n",
    "accuracy_updated = accuracy_score(Y_test, Y_pred_updated)\n",
    "report_updated = classification_report(Y_test, Y_pred_updated)  # Actual Y and the predicted (Y_hat)\n",
    "print(report_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN is not efficient when handling large number of records or large data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAPPLY logistic Regression to compare results with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logistic regression model\n",
    "model_updated = LogisticRegression(n_jobs=-1)\n",
    "model_updated.fit(X_train_updated, Y_train)\n",
    "# Make predictions on the test set\n",
    "Y_pred_updated = model_updated.predict(X_test_updated)\n",
    "# Evaluate the model on the test set\n",
    "accuracy_updated = accuracy_score(Y_test, Y_pred_updated)\n",
    "report_updated = classification_report(Y_test, Y_pred_updated)  # Actual Y and the predicted (Y_hat)\n",
    "print(report_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The KKN didn't perform well as expected, this is because of the dataset structure, the accuracy went to 62% in KNN while the Logistic Regression is kept 66%. Even after dropping the USMER and MEDICAL_UNIT which proves that the USMER and MEDICAL_UNIT are not effective features too. Therefore, they can be dropped. Also keep in mind the time taken in KNN is too much compared to logistic regression since the KNN is not well suited for large datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are versatile machine learning algorithms that can be used for both classification and regression tasks. Unlike models that make predictions based on a linear assumption of the relationship between input variables, Decision Trees make predictions by learning simple decision rules inferred from the data features.\n",
    "\n",
    "**Input:**\n",
    "- Predictor Variables (Features): Similar to other machine learning algorithms, Decision Trees use predictor variables (X) to make decisions. These variables describe the characteristics of the data points and are used by the tree to split the data into subsets.\n",
    "- Target Variable (Label): The target variable (Y) can be either categorical for classification tasks or continuous for regression tasks. Decision Trees make their predictions based on the values of the target variable in the leaf nodes.\n",
    "\n",
    "**Output:**\n",
    "- Class Membership (for classification): In classification tasks, Decision Trees output the class that is most frequent among the instances in the leaf node reached by the new data point.\n",
    "- Value Prediction (for regression): In regression tasks, Decision Trees predict the value for the new data point based on the average value of the instances in the leaf node it falls into.\n",
    "\n",
    "**How It Works:**\n",
    "- Node Splitting: At each node in the tree, the algorithm selects the best predictor variable and splits its value in a way that results in the most significant increase in homogeneity for the target variable in the resulting subsets. The \"best\" split is determined based on a criterion such as Gini impurity for classification or variance reduction for regression.\n",
    "- Tree Building: The process of node splitting continues recursively, creating a tree structure, until a stopping criterion is met. This criterion could be a maximum depth of the tree, a minimum number of samples required to split a node, or a minimum number of samples required for a leaf node, among others.\n",
    "- Pruning: To prevent overfitting, trees are often pruned. Pruning involves cutting back parts of the tree (removing subtrees) to improve its generalization to unseen data. There are several approaches to pruning, such as cost complexity pruning.\n",
    "- Decision Making:\n",
    "    - Classification: For classification tasks, a decision tree assigns a class to a new data point based on the majority class in the leaf node that the data point falls into.\n",
    "    - Regression: For regression tasks, a decision tree predicts the value for a new data point based on the average (or another aggregate measure) of the target variable for the instances in the leaf node it falls into.\n",
    "- Prediction: Once the tree is built (and possibly pruned), it can be used to make predictions. The new data point is passed down the tree until it reaches a leaf node. The prediction is then based on the output defined for that leaf node.\n",
    "\n",
    "**Advantages of Decision Trees:**\n",
    "- Decision Trees are easy to understand and interpret, making them attractive for operational use.\n",
    "- They require little data preparation compared to many other algorithms (e.g., no need for feature scaling).\n",
    "- The tree structure allows for the handling of both numerical and categorical data.\n",
    "\n",
    "**Disadvantages of Decision Trees:**\n",
    "- They are prone to overfitting, especially with complex trees. Pruning and setting the maximum depth of the tree are common ways to address this issue.\n",
    "- Decision Trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using ensemble methods like Random Forests.\n",
    "- They can create biased trees if some classes dominate. It's often recommended to balance the dataset before training a Decision Tree.\n",
    "\n",
    "Decision Trees are a fundamental component of many ensemble methods, such as Random Forests and Gradient Boosting Machines, which combine the predictions of multiple trees to improve prediction accuracy and robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the decision tree\n",
    "model_updated = DecisionTreeClassifier()\n",
    "model_updated.fit(X_train_updated, Y_train)\n",
    "# Make predictions on the test set\n",
    "Y_pred_updated = model_updated.predict(X_test_updated)\n",
    "# Evaluate the model on the test set\n",
    "accuracy_updated = accuracy_score(Y_test, Y_pred_updated)\n",
    "report_updated = classification_report(Y_test, Y_pred_updated)  # Actual Y and the predicted (Y_hat)\n",
    "print(report_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**:\n",
    "- Expecting the Random Forest to perform equal or better than the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'random-forest.jpg' width = 600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RF\n",
    "model_updated = RandomForestClassifier(n_estimators = 10)\n",
    "model_updated.fit(X_train_updated, Y_train)\n",
    "# Predict function\n",
    "Y_pred_updated = model_updated.predict(X_test_updated)\n",
    "# Evaluation of model\n",
    "accuracy_updated = accuracy_score(Y_test, Y_pred_updated)\n",
    "report_updated = classification_report(Y_test, Y_pred_updated)  # Actual Y and the predicted (Y_hat)\n",
    "print(report_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Random forest performed same as decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "- Apply the random forset algorithm with best hyperparameters!\n",
    "    - Note: You can use GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 60}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'parralel_comp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dill\n",
    "dill.load_session('4may2024.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Multithreading Using ThreadPoolExecutor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I/O-Bound Tasks: If your tasks involve a lot of waiting for I/O operations (such as reading from disk or network operations), ThreadPoolExecutor can be very effective. \n",
    "- Threads are lightweight and managed within a single process, which allows you to perform multiple I/O operations in parallel without consuming much in terms of CPU resources.\n",
    "- Integration with asyncio: If you are using asyncio for asynchronous programming, threading can be integrated more seamlessly compared to using multiple processes.\n",
    "- Simplicity: Using ThreadPoolExecutor can be simpler when you want to parallelize tasks that don't require heavy CPU usage or when the libraries you use release the Python GIL during intensive computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model and measure performance\n",
    "def train_model(model, model_name, X_train, Y_train, X_test, Y_test):\n",
    "    start_time = time.time() # Timer start\n",
    "    model.fit(X_train[:30000], Y_train[:30000]) # Fitting (for simplicity 30000)\n",
    "    Y_pred = model.predict(X_test[:10000]) # Predicting (for simplicity 10000)\n",
    "    accuracy = accuracy_score(Y_test[:10000], Y_pred)\n",
    "    elapsed_time = time.time() - start_time # ET now - start_time\n",
    "    return model_name, accuracy, elapsed_time\n",
    "\n",
    "# Asynchronous execution of model training\n",
    "def async_train_models(X_train, Y_train, X_test, Y_test):\n",
    "    models = [\n",
    "        (KNN(), \"KNN\"),\n",
    "        (DecisionTreeClassifier(), \"Decision Tree\"),\n",
    "        (RandomForestClassifier(), \"Random Forest\"),\n",
    "        (LogisticRegression(), \"Logistic Regression\")\n",
    "    ]\n",
    "    with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "        futures = [executor.submit(train_model, model, name, X_train, Y_train, X_test, Y_test) for model, name in models]\n",
    "        for future in futures:\n",
    "            model_name, accuracy, time_taken = future.result()\n",
    "            print(f\"{model_name}: Accuracy = {accuracy:.2f}, Time = {time_taken:.2f} seconds\")\n",
    "\n",
    "# Call the function\n",
    "async_train_models(X_train_updated, Y_train, X_test_updated, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Multiprocessing Using joblib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CPU-Bound Tasks: For tasks that are CPU-intensive, such as training different machine learning models, joblib is generally superior because it can efficiently parallelize these tasks across multiple cores and even multiple machines. \n",
    "- joblib is particularly optimized for these kinds of jobs and is frequently used in scientific computing and data processing tasks.\n",
    "- Memory Management: joblib also has optimizations for memory management when duplicating data across processes. It can avoid copying all data in memory for each process, using shared memory for large numpy arrays.\n",
    "- Scikit-learn Integration: joblib is tightly integrated with scikit-learn, meaning many of its functions and models support joblib for parallel processing out-of-the-box. This makes it an excellent choice for machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model and measure performance\n",
    "def train_model(model, model_name, X_train, Y_train, X_test, Y_test):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return model_name, accuracy, elapsed_time\n",
    "# Asynchronous execution of model training\n",
    "def async_train_models(X_train, Y_train, X_test, Y_test):\n",
    "    models = [\n",
    "        (KNN(), \"KNN\"),\n",
    "        (DecisionTreeClassifier(), \"Decision Tree\"),\n",
    "        (RandomForestClassifier(), \"Random Forest\"),\n",
    "        (LogisticRegression(), \"Logistic Regression\")\n",
    "    ]\n",
    "    results = Parallel(n_jobs=len(models))(delayed(train_model)(model, name, X_train[:30000], Y_train[:30000], X_test[:10000], Y_test[:10000]) for model, name in models)\n",
    "    for result in results:\n",
    "        model_name, accuracy, time_taken = result\n",
    "        print(f\"{model_name}: Accuracy = {accuracy:.2f}, Time = {time_taken:.2f} seconds\")\n",
    "        \n",
    "async_train_models(X_train_updated, Y_train, X_test_updated, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance:\n",
    "- For CPU-bound tasks like training machine learning models, joblib will usually provide better performance than ThreadPoolExecutor because:\n",
    "\n",
    "- Concurrency Model: joblib uses processes instead of threads. This avoids issues with the Global Interpreter Lock (GIL) that can hinder performance in multi-threaded, CPU-bound applications. Processes allow full parallel execution on multiple CPU cores.\n",
    "- Efficient Use of CPUs: By spreading tasks across different cores, joblib ensures that each process is running in true parallel, thus maximizing CPU utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "- If the task is CPU-intensive and involves data-intensive computations typical of model training scenarios, joblib is generally the better choice. It is optimized for such use cases and can efficiently use all available CPU resources.\n",
    "\n",
    "- On the other hand, if the tasks are more I/O-bound or involve significant asynchronous operations, ThreadPoolExecutor might be more appropriate due to its lightweight nature and easier integration with asynchronous I/O operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitocluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
